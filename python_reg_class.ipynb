{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, don't try to use anything except sklearn for trivial problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crx_data = pd.read_csv('crx.data.txt', sep=',', header=None, na_values='?')\n",
    "\n",
    "# name features\n",
    "crx_data.columns = ['A'+str(i+1) for i in range(crx_data.shape[1])]\n",
    "crx_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crx_data[['A11', 'A15']] = crx_data[['A11', 'A15']].astype(float)\n",
    "crx_data.dropna(inplace=True)\n",
    "crx_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = crx_data.iloc[:, :-1]\n",
    "\n",
    "y = crx_data.iloc[:, -1] # target variable\n",
    "\n",
    "\n",
    "# processing categorical features (OneHotEnconding)\n",
    "X = pd.get_dummies(X, prefix=[column for column in X.columns if X[column].dtype == object], \n",
    "                   drop_first=True)\n",
    "\n",
    "# make train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=10)\n",
    "\n",
    "# standartization\n",
    "from sklearn.preprocessing import StandardScaler # remember what it makes?\n",
    "\n",
    "scaler_train = StandardScaler().fit(X_train)\n",
    "X_train = scaler_train.transform(X_train) \n",
    "X_test = scaler_train.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, read this article: https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-5-ensembles-of-algorithms-and-random-forest-8e05246cbba7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100, criterion='gini', max_features='sqrt', \n",
    "                                       n_jobs=4, random_state=0, bootstrap=True, max_depth=10, \n",
    "                                       min_samples_split=30)\n",
    "random_forest.fit(X_train, y_train)\n",
    "rf_predict = random_forest.predict(X_test)\n",
    "\n",
    "# for i, c in enumerate(X.columns[:-1]):\n",
    "#     print(c + ': ' + '%.4f' % random_forest.feature_importances_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest.estimators_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, read this article: https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-10-gradient-boosting-c751538131ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hostel_data = pd.read_csv(\"hostel_factors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = {\"f1\":\"Stuff\",\n",
    "\"f2\":\"Hostel booking\",\n",
    "\"f3\":\"Check-in & Check-out\",\n",
    "\"f4\":\"Room condition\",\n",
    "\"f5\":\"Kitchen condition\",\n",
    "\"f6\":\"Global condition\",\n",
    "\"f7\":\"Additional services\",\n",
    "\"f8\":\"Facilities\",\n",
    "\"f9\":\"Price/quality\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = hostel_data.drop(['hostel', 'rating'], axis=1)\n",
    "y = hostel_data['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gradient_boosting = GradientBoostingRegressor(loss='huber', learning_rate=.01, n_estimators=1000, \n",
    "                                               max_depth=1, subsample=.6, max_features=None, random_state=0)\n",
    "\n",
    "gradient_boosting.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(gradient_boosting.oob_improvement_)), gradient_boosting.oob_improvement_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(gradient_boosting.train_score_)), gradient_boosting.train_score_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to measure quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we need to Google a little bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to fit model in the best way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create lists to save the values of accuracy on training and test sets\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "temp_train_acc = []\n",
    "temp_test_acc = []\n",
    "trees_grid = [5, 10, 15, 20, 30, 50, 75, 100]\n",
    "\n",
    "# Train on the training set\n",
    "for ntrees in trees_grid:\n",
    "    rfc = RandomForestClassifier(n_estimators=ntrees, random_state=42, n_jobs=-1, oob_score=True)\n",
    "    temp_train_acc = []\n",
    "    temp_test_acc = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        scaler_train = StandardScaler().fit(X_train)\n",
    "        X_train = scaler_train.transform(X_train) \n",
    "        X_test = scaler_train.transform(X_test)\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rfc.fit(X_train, y_train)\n",
    "        temp_train_acc.append(rfc.score(X_train, y_train))\n",
    "        temp_test_acc.append(rfc.score(X_test, y_test))\n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)\n",
    "    \n",
    "train_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\n",
    "print(\"Best accuracy on CV is {:.2f}% with {} trees\".format(max(test_acc.mean(axis=1))*100, \n",
    "                                                        trees_grid[np.argmax(test_acc.mean(axis=1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(trees_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\n",
    "ax.plot(trees_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\n",
    "ax.fill_between(trees_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\n",
    "ax.fill_between(trees_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim([0.8,1.02])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"N_estimators\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What features are important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = gradient_boosting.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# Plot the feature importancies of the forest\n",
    "num_to_plot = 5\n",
    "feature_indices = [ind+1 for ind in indices[:num_to_plot]]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(num_to_plot):\n",
    "    print(\"%d. %s %f \" % (f + 1, \n",
    "            features[\"f\"+str(feature_indices[f])], \n",
    "            importances[indices[f]]))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Feature importances\")\n",
    "bars = plt.bar(range(num_to_plot), \n",
    "               importances[indices[:num_to_plot]],\n",
    "       color=([str(i/float(num_to_plot+1)) \n",
    "               for i in range(num_to_plot)]),\n",
    "               align=\"center\")\n",
    "ticks = plt.xticks(range(num_to_plot), \n",
    "                   feature_indices)\n",
    "plt.xlim([-1, num_to_plot])\n",
    "plt.legend(bars, [u''.join(features[\"f\"+str(i)]) \n",
    "                  for i in feature_indices]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to do next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course unsupervised learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
